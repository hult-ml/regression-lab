{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple and Polynomial Regression with examples\n",
        "\n",
        "Originally created by \n",
        "\n",
        "**Authors:** Rahul Dave, David Sondak, Will Claybaugh, Pavlos Protopapas\n",
        "\n",
        "Make sure you have the following packages: jupyterlab scikit-learn pandas matplotlib numpy seaborn . Colab is probably a good place to run this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "<ol start=\"0\">\n",
        "<li> Learning Goals </li>\n",
        "<li> Polynomial Regression, and  Cab Data</li>\n",
        "<li> Multiple regression and exploring some Football data </li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "`sklearn` is focused on the _prediction_ task: given \\[new\\] data, guess what the response value is. As a result, statsmodels has lots of tools to discuss confidence, but isn't great at dealing with test sets. Sklearn is great at test sets and validations, but can't really discuss uncertainty in the parameters or predictions. In short:\n",
        "\n",
        "  - sklearn is about putting a line through it and predicting new values using that line. If the line gives good predictions on the test set, who cares about anything else?\n",
        "  - Another package called statsmodels (not used here) assumes more about how the data were generated, and (if the assumptions are correct) can tell you about uncertainty in the results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regression metrics\n",
        "\n",
        "- **mean squared error**\n",
        "- **R-squared**: An interpretable summary of how well the model did. 1 is perfect, 0 is a trivial baseline model, negative is worse than the trivial model. Look it up.\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Polynomial Regression, and the Cab Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# read in the data, break into train and test\n",
        "cab_df = pd.read_csv(\"data/dataset_1.txt\")\n",
        "train_data, test_data = train_test_split(cab_df, test_size=.2, random_state=42)\n",
        "cab_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "cab_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do some data cleaning\n",
        "X_train = train_data['TimeMin'].values.reshape(-1,1)/60\n",
        "y_train = train_data['PickupCount'].values\n",
        "\n",
        "X_test = test_data['TimeMin'].values.reshape(-1,1)/60\n",
        "y_test = test_data['PickupCount'].values\n",
        "\n",
        "\n",
        "def plot_cabs(cur_model, poly_transformer=None):\n",
        "    \n",
        "    # build the x values for the prediction line\n",
        "    x_vals = np.arange(0,24,.1).reshape(-1,1)\n",
        "    \n",
        "    # if needed, build the design matrix\n",
        "    if poly_transformer:\n",
        "        design_mat = poly_transformer.fit_transform(x_vals)\n",
        "    else:\n",
        "        design_mat = x_vals\n",
        "    \n",
        "    # make the prediction at each x value\n",
        "    prediction = cur_model.predict(design_mat)\n",
        "    \n",
        "    # plot the prediction line, and the test data\n",
        "    plt.plot(x_vals,prediction, '.-', color='k', label=\"Prediction\")\n",
        "    plt.scatter(X_test, y_test, label=\"Test Data\")\n",
        "\n",
        "    # label your plots\n",
        "    plt.ylabel(\"Number of Taxi Pickups\")\n",
        "    plt.xlabel(\"Time of Day (Hours Past Midnight)\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "fitted_cab_model0 = LinearRegression().fit(X_train, y_train)\n",
        "plot_cabs(fitted_cab_model0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "fitted_cab_model0.score(X_test, y_test) # computes R^2 for you"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there's still a lot of variation in cab pickups that's not being caught by a linear fit. And the linear fit is predicting massively more pickups at 11:59pm than at 12:00am. However, we can add columns to our design matrix for $TimeMin^2$ and $TimeMin^3$ and so on, allowing a wigglier polynomial that will better fit the data.\n",
        "\n",
        "We'll be using sklearn's `PolynomialFeatures` to take some of the tedium out of building the new design matrix. In fact, if all we want is a formula like $y \\approx \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ...$ it will directly return the new design matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "tra = PolynomialFeatures(3, include_bias=True)\n",
        "xx1 = np.linspace(0,1, 5)\n",
        "xx2 = np.linspace(9,10, 5)\n",
        "xx1, xx2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "xx3 = np.concatenate([xx1.reshape(-1,1),xx2.reshape(-1,1)], axis = 1)\n",
        "xx3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "tra = PolynomialFeatures(2, include_bias=False)\n",
        "tra.fit_transform(xx3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer_3 = PolynomialFeatures(3, include_bias=False)\n",
        "new_features = transformer_3.fit_transform(X_train)\n",
        "new_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few notes on `PolynomialFeatures`:\n",
        "\n",
        "- The interface is a bit strange. `PolynomialFeatures` is a 'transformer' in sklearn. We'll be using several transformers that learn a transformation on the training data and then apply that transformation on future data. On these (more typical) transformers it makes sense to have a `.fit()` and a separate `.transform()`. With PolynomialFeatures, the `.fit()` is pretty trivial, and we often fit and transform in one command, as seen above.\n",
        "- You rarely want to `include_bias` (a column of all 1s), since sklearn will add it automatically and statsmodels can just `add_constant` right before you fit to the design matrix\n",
        "- If you want polynomial features for a several different variables, you should call `.fit_transform()` separately on each column and append all the results to the design matrix (unless you also want interaction terms between the newly-created features). See `np.concatenate` for joining arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "fitted_cab_model3 = LinearRegression().fit(new_features, y_train)\n",
        "plot_cabs(fitted_cab_model3, transformer_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"exercise\"><b>Exercise</b></div>\n",
        "\n",
        "**Questions**:\n",
        "1. Calculate the polynomial model's $R^2$ performance on the test set. \n",
        "2. Does the polynomial model improve on the purely linear model?\n",
        "3. Make a residual plot for the polynomial model. What does this plot tell us about the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*your answer here*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Other features\n",
        "Polynomial features are not the only constucted features that help fit the data. Because these data have a 24 hour cycle, we may want to build features that follow such a cycle. For example, $sin(24\\frac{x}{2\\pi})$, $sin(12\\frac{x}{2\\pi})$, $sin(8\\frac{x}{2\\pi})$. Other feature transformations are appropriate to other types of data. For instance certain feature transformations have been developed for geographical data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Multiple regression and exploring the Football data\n",
        "\n",
        "Let's move on to a truly interesting dataset. The data imported below were scraped by [Shubham Maurya](https://www.kaggle.com/mauryashubham/linear-regression-to-predict-market-value/data) and record various facts about players in the English Premier League. Our goal will be to fit models that predict the players' market value (what the player could earn when hired by a new team), as estimated by transfermrkt.com.\n",
        "\n",
        "`name`: Name of the player  \n",
        "`club`: Club of the player  \n",
        "`age` : Age of the player  \n",
        "`position` : The usual position on the pitch  \n",
        "`position_cat` :  1 for attackers, 2 for midfielders, 3 for defenders, 4 for goalkeepers  \n",
        "`market_value` : As on transfermrkt.com on July 20th, 2017  \n",
        "`page_views` : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017  \n",
        "`fpl_value` : Value in Fantasy Premier League as on July 20th, 2017  \n",
        "`fpl_sel` : % of FPL players who have selected that player in their team  \n",
        "`fpl_points` : FPL points accumulated over the previous season  \n",
        "`region`: 1 for England, 2 for EU, 3 for Americas, 4 for Rest of World  \n",
        "`nationality`: Player's nationality  \n",
        "`new_foreign`: Whether a new signing from a different league, for 2017/18 (till 20th July)  \n",
        "`age_cat`: a categorical version of the Age feature  \n",
        "`club_id`: a numerical version of the Club feature  \n",
        "`big_club`: Whether one of the Top 6 clubs  \n",
        "`new_signing`: Whether a new signing for 2017/18 (till 20th July)  \n",
        "\n",
        "As always, we first import, verify, split, and explore the data.\n",
        "\n",
        "## Part 2.1: Import and verification and grouping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "league_df = pd.read_csv(\"data/league_data.txt\")\n",
        "print(league_df.dtypes)\n",
        "league_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "league_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "league_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Stratified) train/test split\n",
        "We want to make sure that the training and test data have appropriate representation of each region; it would be bad for the training data to entirely miss a region. This is especially important because some regions are rather rare.\n",
        "\n",
        "<div class=\"exercise\"><b>Exercise</b></div>\n",
        "\n",
        "**Questions**:\n",
        "\n",
        "1. Use the `train_test_split` function to and its 'stratify' argument to split the data, keeping equal representation of each region (This will not work out of the box on this data. Deal with the resulting issue).\n",
        "2. Deal with the issue you encountered above.\n",
        "3. How did you deal with the error generated by `train_test_split`? How did you justify your action? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*your answer here*:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data.shape, test_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we won't be peeking at the test set, let's explore and look for patterns! We'll introduce a number of useful pandas and numpy functions along the way. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Groupby\n",
        "Pandas' `.groupby()` function is a wonderful tool for data analysis. It allows us to analyze each of several subgroups.\n",
        "\n",
        "Many times, `.groupby()` is combined with `.agg()` to get a summary statistic for each subgroup. For instance: What is the average market value, median page views, and maximum fpl for each player position?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data.groupby('position').agg({\n",
        "    'market_value': np.mean,\n",
        "    'page_views': np.median,\n",
        "    'fpl_points': np.max\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data.position.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data.groupby(['big_club', 'position']).agg({\n",
        "    'market_value': np.mean,\n",
        "    'page_views': np.mean,\n",
        "    'fpl_points': np.mean\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2.2: Linear regression on the football data\n",
        "\n",
        "This section focuses on fitting a model to the football data. The model we'll use is\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{market-value}  = \\beta_0 + \\beta_1 \\text{fpl-points} \\\\\n",
        " + \\beta_2\\text{age} + \\beta_3\\text{age}^2 \\\\\n",
        " + \\beta_4log_2\\left(\\text{page-views}\\right) + \\beta_5\\text{new-signing} \\\\\n",
        " + \\beta_6\\text{big-club} + \\beta_7\\text{position-cat}\n",
        "$$\n",
        "\n",
        "We're including a 2nd degree polynomial in age because we expect pay to increase as a player gains experience, but then decrease as they continue aging. We're taking the log of page views because they have such a large, skewed range and the transformed variable will have fewer outliers that could bias the line. We choose the base of the log to be 2 just to make interpretation cleaner.\n",
        "\n",
        "<div class=\"exercise\"><b>Exercise</b></div>\n",
        "\n",
        "**Questions**:\n",
        "\n",
        "1. Build the $X$ matrix function and fit this model to the training data. How good is the overall model?\n",
        "2. What should a player do in order to improve their market value? How many page views should a player go get to increase their market value by 10?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*your answer here*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "agecoef = fitted_model_1.coef_[1]\n",
        "age2coef = fitted_model_1.coef_[2]\n",
        "agecoef, age2coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_vals = np.linspace(-100,100,1000)\n",
        "y_vals = agecoef*x_vals +age2coef*x_vals**2\n",
        "plt.plot(x_vals, y_vals)\n",
        "plt.title(\"Effect of Age\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Contribution to Predicted Market Value\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 2.3: Turning Categorical Variables into multiple binary variables\n",
        "\n",
        "Of course, we have an error in how we've included player position. Even though the variable is numeric (1,2,3,4) and the model runs without issue, the value we're getting back is garbage. The interpretation, such as it is, is that there is an equal effect of moving from position category 1 to 2, from 2 to 3, and from 3 to 4, and that this effect is about -.61.\n",
        "\n",
        "In reality, we don't expect moving from one position category to another to be equivalent, nor for a move from category 1 to category 3 to be twice as important as a move from category 1 to category 2. We need to introduce better features to model this variable.\n",
        "\n",
        "We'll use `pd.get_dummies` to do the work for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_design_recoded = pd.get_dummies(train_design, columns=['position_cat'], drop_first=True)\n",
        "test_design_recoded = pd.get_dummies(test_design, columns=['position_cat'], drop_first=True)\n",
        "\n",
        "train_design_recoded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've removed the original `position_cat` column and created three new ones.\n",
        "\n",
        "#### Why only three new columns?\n",
        "Why does pandas give us the option to drop the first category? \n",
        "\n",
        "<div class=\"exercise\"><b>Exercise</b></div>\n",
        "\n",
        "**Questions**:\n",
        "\n",
        "1. If we're fitting a model without a constant, should we have three dummy columns or four dummy columns?\n",
        "2. Fit a model and interpret the coefficient of `position_cat_2`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "resu = LinearRegression().fit(train_design_recoded, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "r2_score(y_test, resu.predict(test_design_recoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_design_recoded.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answers**:\n",
        "1. If our model does not have a constant, we must include all four dummy variable columns. If we drop one, we're not modeling any effect of being in that category, and effectively assuming the dropped category's effect is 0.\n",
        "2. Being in position 2 (instead of position 1) has an impact between -1.54 and +2.38 on a player's market value. Since we're using an intercept, the dropped category becomes the baseline and the effect of any dummy variable is the effect of being in that category instead of the baseline category."
      ]
    }
  ]
}